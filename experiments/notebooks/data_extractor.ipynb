{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7e179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import asyncio\n",
    "from enum import Enum\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dab343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMProvider(Enum):\n",
    "    OPENAI=\"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    COHERE= \"cohere\"\n",
    "    LOCAL = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ee8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnhancedResumeData:\n",
    "    \"\"\"Enhanced resume data with LLM-extracted insights\"\"\"\n",
    "    # SpaCy extracted data\n",
    "    basic_info: dict[str, any] = field(default_factory=dict)\n",
    "    \n",
    "    # LLM enhanced data\n",
    "    skills_categorized: dict[str, list[str]] = field(default_factory=dict)  \n",
    "    experience_insights: list[dict[str, any]] = field(default_factory=list)\n",
    "    achievement_metrics: list[dict[str, any]] = field(default_factory=list)\n",
    "    career_progression: dict[str, any] = field(default_factory=dict)\n",
    "    personality_traits: list[str] = field(default_factory=list)\n",
    "    industry_fit: dict[str, float] = field(default_factory=dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0e7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnhancedEmailData:\n",
    "    \"\"\"Enhanced email data with LLM analysis\"\"\"\n",
    "    # SpaCy extracted data\n",
    "    basic_structure: dict[str, any] = field(default_factory=dict)\n",
    "    \n",
    "    # LLM enhanced data\n",
    "    emotional_tone: dict[str, float] = field(default_factory=dict)  # professional, friendly, urgent, etc.\n",
    "    intent_hierarchy: list[dict[str, any]] = field(default_factory=list)  # primary, secondary intents\n",
    "    action_items: list[str] = field(default_factory=list)\n",
    "    stakeholders: list[dict[str, any]] = field(default_factory=list)\n",
    "    follow_up_required: bool = False\n",
    "    priority_level: str = \"medium\"\n",
    "    relationship_context: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde11498",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnhancedScientificData:\n",
    "    \"\"\"Enhanced scientific paper data with LLM insights\"\"\"\n",
    "    # SpaCy extracted data\n",
    "    basic_structure: dict[str, any] = field(default_factory=dict)\n",
    "    \n",
    "    # LLM enhanced data\n",
    "    research_contribution: dict[str, any] = field(default_factory=dict)\n",
    "    methodology_type: str = \"\"\n",
    "    novelty_assessment: dict[str, float] = field(default_factory=dict)\n",
    "    research_gaps_identified: list[str] = field(default_factory=list)\n",
    "    future_work_suggestions: list[str] = field(default_factory=list)\n",
    "    interdisciplinary_connections: list[str] = field(default_factory=list)\n",
    "    reproducibility_score: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138b22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ContactInfo:\n",
    "    \"\"\"Structure for contact information\"\"\"\n",
    "    emails: list[str] = field(default_factory=list)\n",
    "    phones: list[str] = field(default_factory=list)\n",
    "    addresses: list[str] = field(default_factory=list)\n",
    "    websites: list[str] = field(default_factory=list)\n",
    "    social_profiles: dict[str, str] = field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d9e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ad89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ResumeData:\n",
    "    \"\"\"Structured resume data\"\"\"\n",
    "    personal_info: ContactInfo = field(default_factory=ContactInfo)\n",
    "    summary: str = \"\"\n",
    "    skills: list[str] = field(default_factory=list)\n",
    "    experience: list[dict[str, any]] = field(default_factory=list)\n",
    "    education: list[dict[str, any]] = field(default_factory=list)\n",
    "    certifications: list[str] = field(default_factory=list)\n",
    "    languages: list[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class EmailData:\n",
    "    \"\"\"Structured email data\"\"\"\n",
    "    sender: str = \"\"\n",
    "    recipients: list[str] = field(default_factory=list)\n",
    "    cc: list[str] = field(default_factory=list)\n",
    "    bcc: list[str] = field(default_factory=list)\n",
    "    subject: str = \"\"\n",
    "    date: datetime | None = None\n",
    "    body: str = \"\"\n",
    "    attachments: list[str] = field(default_factory=list)\n",
    "    sentiment: str = \"\"\n",
    "    intent: str = \"\"\n",
    "    entities: list[dict[str, any]] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd0f5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScientificPaperData:\n",
    "    \"\"\"Structured scientific paper data\"\"\"\n",
    "    title: str = \"\"\n",
    "    authors: list[str] = field(default_factory=list)\n",
    "    affiliations: list[str] = field(default_factory=list)\n",
    "    abstract: str = \"\"\n",
    "    keywords: list[str] = field(default_factory=list)\n",
    "    sections: dict[str, str] = field(default_factory=dict)\n",
    "    citations: list[str] = field(default_factory=list)\n",
    "    references: list[str] = field(default_factory=list)\n",
    "    figures_tables: list[str] = field(default_factory=list)\n",
    "    doi: str = \"\"\n",
    "    journal: str = \"\"\n",
    "    publication_date: datetime | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d08348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7e6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"custom_entity_ruler\")\n",
    "def custom_entity_ruler(doc):\n",
    "    \"\"\"Custom component for domain-specific entity recognition\"\"\"\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb87d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f0fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentParser:\n",
    "    \"\"\"Base class for document parsing using SpaCy building blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        \"\"\"Initialize the parser with SpaCy components\"\"\"\n",
    "        # Load Language model\n",
    "        self.nlp = spacy.load(model_name)\n",
    "        \n",
    "        # Initialize matchers\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.phrase_matcher = PhraseMatcher(self.nlp.vocab)\n",
    "        \n",
    "        # Setup custom patterns and rules\n",
    "        self._setup_patterns()\n",
    "\n",
    "        # Add custom pipeline components\n",
    "        if \"custom_entity_ruler\" not in self.nlp.pipe_names:\n",
    "            self.nlp.add_pipe(\"custom_entity_ruler\", last=True)\n",
    "    \n",
    "    def _setup_patterns(self):\n",
    "        \"\"\"Setup common patterns for all document types\"\"\"\n",
    "        # Email patterns\n",
    "        email_pattern = [{\"LIKE_EMAIL\": True}]\n",
    "        self.matcher.add(\"EMAIL\", [email_pattern])\n",
    "        \n",
    "        # Phone patterns\n",
    "        phone_patterns = [\n",
    "            [{\"SHAPE\": \"ddd-ddd-dddd\"}],\n",
    "            [{\"SHAPE\": \"(ddd) ddd-dddd\"}],\n",
    "            [{\"TEXT\": {\"REGEX\": r\"\\+?\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\"}}]\n",
    "        ]\n",
    "        self.matcher.add(\"PHONE\", phone_patterns)\n",
    "        \n",
    "        # URL patterns\n",
    "        url_pattern = [{\"LIKE_URL\": True}]\n",
    "        self.matcher.add(\"URL\", [url_pattern])\n",
    "        \n",
    "        # Date patterns\n",
    "        date_patterns = [\n",
    "            [{\"SHAPE\": \"dd/dd/dddd\"}],\n",
    "            [{\"SHAPE\": \"dd-dd-dddd\"}],\n",
    "            [{\"ENT_TYPE\": \"DATE\"}]\n",
    "        ]\n",
    "        self.matcher.add(\"DATE\", date_patterns)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters that might interfere with parsing\n",
    "        text = re.sub(r'[^\\w\\s@.-]', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_entities(self, doc: Doc) -> list[dict[str, any]]:\n",
    "        \"\"\"Extract named entities using SpaCy's EntityRecognizer\"\"\"\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char,\n",
    "                'confidence': getattr(ent, 'kb_id_', 0.0)\n",
    "            })\n",
    "        return entities\n",
    "    \n",
    "    def extract_contact_info(self, doc: Doc) -> ContactInfo:\n",
    "        \"\"\"Extract contact information using matchers\"\"\"\n",
    "        contact = ContactInfo()\n",
    "        \n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            label = self.nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]\n",
    "            \n",
    "            if label == \"EMAIL\":\n",
    "                contact.emails.append(span.text)\n",
    "            elif label == \"PHONE\":\n",
    "                contact.phones.append(span.text)\n",
    "            elif label == \"URL\":\n",
    "                contact.websites.append(span.text)\n",
    "        \n",
    "        return contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a7d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeParser(DocumentParser):\n",
    "    \"\"\"Specialized parser for resumes\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        super().__init__(model_name)\n",
    "        self._setup_resume_patterns()\n",
    "    \n",
    "    def _setup_resume_patterns(self):\n",
    "        \"\"\"Setup resume-specific patterns\"\"\"\n",
    "        # Skills section patterns\n",
    "        skills_headers = [\"skills\", \"technical skills\", \"core competencies\", \"expertise\"]\n",
    "        skills_patterns = [[{\"LOWER\": {\"IN\": skills_headers}}]]\n",
    "        self.matcher.add(\"SKILLS_HEADER\", skills_patterns)\n",
    "        \n",
    "        # Experience section patterns\n",
    "        exp_headers = [\"experience\", \"work experience\", \"employment\", \"professional experience\"]\n",
    "        exp_patterns = [[{\"LOWER\": {\"IN\": exp_headers}}]]\n",
    "        self.matcher.add(\"EXPERIENCE_HEADER\", exp_patterns)\n",
    "        \n",
    "        # Education section patterns\n",
    "        edu_headers = [\"education\", \"academic background\", \"qualifications\"]\n",
    "        edu_patterns = [[{\"LOWER\": {\"IN\": edu_headers}}]]\n",
    "        self.matcher.add(\"EDUCATION_HEADER\", edu_patterns)\n",
    "        \n",
    "        # Job title patterns\n",
    "        job_titles = [\"manager\", \"developer\", \"engineer\", \"analyst\", \"director\", \"specialist\"]\n",
    "        job_patterns = [[{\"LOWER\": {\"IN\": job_titles}}]]\n",
    "        self.matcher.add(\"JOB_TITLE\", job_patterns)\n",
    "    \n",
    "    def parse(self, text: str) -> ResumeData:\n",
    "        \"\"\"Parse resume text and extract structured data\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        resume_data = ResumeData()\n",
    "        \n",
    "        # Extract contact information\n",
    "        resume_data.personal_info = self.extract_contact_info(doc)\n",
    "        \n",
    "        # Extract sections using sentence segmentation\n",
    "        sections = self._identify_sections(doc)\n",
    "        \n",
    "        # Parse each section\n",
    "        for section_name, section_text in sections.items():\n",
    "            section_doc = self.nlp(section_text)\n",
    "            \n",
    "            if \"skill\" in section_name.lower():\n",
    "                resume_data.skills = self._extract_skills(section_doc)\n",
    "            elif \"experience\" in section_name.lower():\n",
    "                resume_data.experience = self._extract_experience(section_doc)\n",
    "            elif \"education\" in section_name.lower():\n",
    "                resume_data.education = self._extract_education(section_doc)\n",
    "            elif \"summary\" in section_name.lower() or \"objective\" in section_name.lower():\n",
    "                resume_data.summary = section_text\n",
    "        \n",
    "        return resume_data\n",
    "    \n",
    "    def _identify_sections(self, doc: Doc) -> dict[str, str]:\n",
    "        \"\"\"Use dependency parsing and pattern matching to identify resume sections\"\"\"\n",
    "        sections = {}\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        section_starts = []\n",
    "        for match_id, start, end in matches:\n",
    "            label = self.nlp.vocab.strings[match_id]\n",
    "            if \"_HEADER\" in label:\n",
    "                section_starts.append((start, label.replace(\"_HEADER\", \"\").lower(), end))\n",
    "        \n",
    "        # Sort by position\n",
    "        section_starts.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Extract text between section headers\n",
    "        for i, (start, section_name, end) in enumerate(section_starts):\n",
    "            next_start = section_starts[i + 1][0] if i + 1 < len(section_starts) else len(doc)\n",
    "            section_text = doc[end:next_start].text\n",
    "            sections[section_name] = section_text\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _extract_skills(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"Extract skills using NER and POS tagging\"\"\"\n",
    "        skills = []\n",
    "        \n",
    "        # Extract noun phrases as potential skills\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if len(chunk.text.split()) <= 3:  # Short phrases likely to be skills\n",
    "                skills.append(chunk.text)\n",
    "        \n",
    "        # Extract technical terms (proper nouns, specific patterns)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PROPN\" and not token.ent_type_:\n",
    "                skills.append(token.text)\n",
    "        \n",
    "        return list(set(skills))  # Remove duplicates\n",
    "    \n",
    "    def _extract_experience(self, doc: Doc) -> list[dict[str, any]]:\n",
    "        \"\"\"Extract work experience using NER and pattern matching\"\"\"\n",
    "        experiences = []\n",
    "        \n",
    "        # Use sentence boundaries to separate different experiences\n",
    "        for sent in doc.sents:\n",
    "            exp_entry = {}\n",
    "            \n",
    "            # Extract organizations\n",
    "            orgs = [ent.text for ent in sent.ents if ent.label_ == \"ORG\"]\n",
    "            if orgs:\n",
    "                exp_entry[\"company\"] = orgs[0]\n",
    "            \n",
    "            # Extract dates\n",
    "            dates = [ent.text for ent in sent.ents if ent.label_ == \"DATE\"]\n",
    "            if dates:\n",
    "                exp_entry[\"duration\"] = dates\n",
    "            \n",
    "            # Extract job titles using pattern matching\n",
    "            matches = self.matcher(sent)\n",
    "            for match_id, start, end in matches:\n",
    "                if self.nlp.vocab.strings[match_id] == \"JOB_TITLE\":\n",
    "                    exp_entry[\"title\"] = sent[start:end].text\n",
    "            \n",
    "            if exp_entry:\n",
    "                exp_entry[\"description\"] = sent.text\n",
    "                experiences.append(exp_entry)\n",
    "        \n",
    "        return experiences\n",
    "    \n",
    "    def _extract_education(self, doc: Doc) -> list[dict[str, any]]:\n",
    "        \"\"\"Extract education information\"\"\"\n",
    "        education = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            edu_entry = {}\n",
    "            \n",
    "            # Extract organizations (schools/universities)\n",
    "            orgs = [ent.text for ent in sent.ents if ent.label_ == \"ORG\"]\n",
    "            if orgs:\n",
    "                edu_entry[\"institution\"] = orgs[0]\n",
    "            \n",
    "            # Extract dates\n",
    "            dates = [ent.text for ent in sent.ents if ent.label_ == \"DATE\"]\n",
    "            if dates:\n",
    "                edu_entry[\"graduation_date\"] = dates[0]\n",
    "            \n",
    "            if edu_entry:\n",
    "                edu_entry[\"description\"] = sent.text\n",
    "                education.append(edu_entry)\n",
    "        \n",
    "        return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3cf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailParser(DocumentParser):\n",
    "    \"\"\"Specialized parser for emails\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        super().__init__(model_name)\n",
    "        self._setup_email_patterns()\n",
    "    \n",
    "    def _setup_email_patterns(self):\n",
    "        \"\"\"Setup email-specific patterns\"\"\"\n",
    "        # Email header patterns\n",
    "        header_patterns = [\n",
    "            [{\"LOWER\": \"from\"}, {\"TEXT\": \":\"}],\n",
    "            [{\"LOWER\": \"to\"}, {\"TEXT\": \":\"}],\n",
    "            [{\"LOWER\": \"subject\"}, {\"TEXT\": \":\"}],\n",
    "            [{\"LOWER\": \"date\"}, {\"TEXT\": \":\"}]\n",
    "        ]\n",
    "        self.matcher.add(\"EMAIL_HEADER\", header_patterns)\n",
    "        \n",
    "        # Greeting patterns\n",
    "        greetings = [\"dear\", \"hello\", \"hi\", \"greetings\"]\n",
    "        greeting_patterns = [[{\"LOWER\": {\"IN\": greetings}}]]\n",
    "        self.matcher.add(\"GREETING\", greeting_patterns)\n",
    "        \n",
    "        # Closing patterns\n",
    "        closings = [\"sincerely\", \"regards\", \"best\", \"thanks\", \"thank you\"]\n",
    "        closing_patterns = [[{\"LOWER\": {\"IN\": closings}}]]\n",
    "        self.matcher.add(\"CLOSING\", closing_patterns)\n",
    "    \n",
    "    def parse(self, text: str) -> EmailData:\n",
    "        \"\"\"Parse email text and extract structured data\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        email_data = EmailData()\n",
    "        \n",
    "        # Extract header information\n",
    "        self._extract_headers(text, email_data)\n",
    "        \n",
    "        # Extract body content\n",
    "        email_data.body = self._extract_body(doc)\n",
    "        \n",
    "        # Extract entities\n",
    "        email_data.entities = self.extract_entities(doc)\n",
    "        \n",
    "        # Analyze sentiment using TextCategorizer (if available)\n",
    "        email_data.sentiment = self._analyze_sentiment(doc)\n",
    "        \n",
    "        # Determine intent\n",
    "        email_data.intent = self._determine_intent(doc)\n",
    "        \n",
    "        return email_data\n",
    "    \n",
    "    def _extract_headers(self, text: str, email_data: EmailData):\n",
    "        \"\"\"Extract email headers using regex patterns\"\"\"\n",
    "        # Simple regex patterns for email headers\n",
    "        from_match = re.search(r'From:\\s*(.+)', text, re.IGNORECASE)\n",
    "        if from_match:\n",
    "            email_data.sender = from_match.group(1).strip()\n",
    "        \n",
    "        to_match = re.search(r'To:\\s*(.+)', text, re.IGNORECASE)\n",
    "        if to_match:\n",
    "            email_data.recipients = [r.strip() for r in to_match.group(1).split(',')]\n",
    "        \n",
    "        subject_match = re.search(r'Subject:\\s*(.+)', text, re.IGNORECASE)\n",
    "        if subject_match:\n",
    "            email_data.subject = subject_match.group(1).strip()\n",
    "    \n",
    "    def _extract_body(self, doc: Doc) -> str:\n",
    "        \"\"\"Extract email body content, removing headers\"\"\"\n",
    "        # Find the start of the actual content (after headers)\n",
    "        body_start = 0\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.text.lower() in ['dear', 'hello', 'hi'] or token.like_email:\n",
    "                body_start = i\n",
    "                break\n",
    "        \n",
    "        return doc[body_start:].text\n",
    "    \n",
    "    def _analyze_sentiment(self, doc: Doc) -> str:\n",
    "        \"\"\"Analyze email sentiment (simplified version)\"\"\"\n",
    "        positive_words = ['thank', 'great', 'excellent', 'good', 'appreciate']\n",
    "        negative_words = ['sorry', 'problem', 'issue', 'urgent', 'disappointed']\n",
    "        \n",
    "        pos_count = sum(1 for token in doc if token.text.lower() in positive_words)\n",
    "        neg_count = sum(1 for token in doc if token.text.lower() in negative_words)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return \"positive\"\n",
    "        elif neg_count > pos_count:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    def _determine_intent(self, doc: Doc) -> str:\n",
    "        \"\"\"Determine email intent using pattern matching\"\"\"\n",
    "        question_indicators = ['?', 'how', 'what', 'when', 'where', 'why', 'can you']\n",
    "        request_indicators = ['please', 'could you', 'would you', 'need']\n",
    "        \n",
    "        text_lower = doc.text.lower()\n",
    "        \n",
    "        if any(indicator in text_lower for indicator in question_indicators):\n",
    "            return \"inquiry\"\n",
    "        elif any(indicator in text_lower for indicator in request_indicators):\n",
    "            return \"request\"\n",
    "        else:\n",
    "            return \"informational\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "486dccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScientificPaperParser(DocumentParser):\n",
    "    \"\"\"Specialized parser for scientific publications\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        super().__init__(model_name)\n",
    "        self._setup_scientific_patterns()\n",
    "    \n",
    "    def _setup_scientific_patterns(self):\n",
    "        \"\"\"Setup scientific paper-specific patterns\"\"\"\n",
    "        # Section headers\n",
    "        section_headers = [\n",
    "            \"abstract\", \"introduction\", \"methodology\", \"methods\", \"results\", \n",
    "            \"discussion\", \"conclusion\", \"references\", \"acknowledgments\"\n",
    "        ]\n",
    "        section_patterns = [[{\"LOWER\": {\"IN\": section_headers}}]]\n",
    "        self.matcher.add(\"SECTION_HEADER\", section_patterns)\n",
    "        \n",
    "        # Citation patterns\n",
    "        citation_patterns = [\n",
    "            [{\"TEXT\": \"(\"}, {\"LIKE_NUM\": True}, {\"TEXT\": \")\"}],\n",
    "            [{\"TEXT\": \"[\"}, {\"LIKE_NUM\": True}, {\"TEXT\": \"]\"}]\n",
    "        ]\n",
    "        self.matcher.add(\"CITATION\", citation_patterns)\n",
    "        \n",
    "        # Figure/Table references\n",
    "        fig_table_patterns = [\n",
    "            [{\"LOWER\": \"figure\"}, {\"LIKE_NUM\": True}],\n",
    "            [{\"LOWER\": \"table\"}, {\"LIKE_NUM\": True}],\n",
    "            [{\"LOWER\": \"fig\"}, {\"TEXT\": \".\"}, {\"LIKE_NUM\": True}]\n",
    "        ]\n",
    "        self.matcher.add(\"FIG_TABLE\", fig_table_patterns)\n",
    "    \n",
    "    def parse(self, text: str) -> ScientificPaperData:\n",
    "        \"\"\"Parse scientific paper and extract structured data\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        paper_data = ScientificPaperData()\n",
    "        \n",
    "        # Extract title (usually first sentence or line)\n",
    "        paper_data.title = self._extract_title(doc)\n",
    "        \n",
    "        # Extract authors and affiliations\n",
    "        paper_data.authors, paper_data.affiliations = self._extract_authors_affiliations(doc)\n",
    "        \n",
    "        # Extract sections\n",
    "        paper_data.sections = self._extract_sections(doc)\n",
    "        \n",
    "        # Extract abstract\n",
    "        if \"abstract\" in paper_data.sections:\n",
    "            paper_data.abstract = paper_data.sections[\"abstract\"]\n",
    "        \n",
    "        # Extract citations and references\n",
    "        paper_data.citations = self._extract_citations(doc)\n",
    "        paper_data.references = self._extract_references(doc)\n",
    "        \n",
    "        # Extract figures and tables\n",
    "        paper_data.figures_tables = self._extract_figures_tables(doc)\n",
    "        \n",
    "        # Extract keywords using NER and noun phrases\n",
    "        paper_data.keywords = self._extract_keywords(doc)\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    def _extract_title(self, doc: Doc) -> str:\n",
    "        \"\"\"Extract paper title (typically the first sentence)\"\"\"\n",
    "        # Find the first sentence that's likely a title\n",
    "        for sent in doc.sents:\n",
    "            if len(sent.text.split()) > 3 and not sent.text.lower().startswith(('abstract', 'keywords')):\n",
    "                return sent.text.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def _extract_authors_affiliations(self, doc: Doc) -> tuple[list[str], list[str]]:\n",
    "        \"\"\"Extract authors and their affiliations using NER\"\"\"\n",
    "        authors = []\n",
    "        affiliations = []\n",
    "        \n",
    "        # Look for person entities in the first few sentences\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            if i > 5:  # Authors usually appear early\n",
    "                break\n",
    "            \n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    authors.append(ent.text)\n",
    "                elif ent.label_ == \"ORG\":\n",
    "                    affiliations.append(ent.text)\n",
    "        \n",
    "        return list(set(authors)), list(set(affiliations))\n",
    "    \n",
    "    def _extract_sections(self, doc: Doc) -> dict[str, str]:\n",
    "        \"\"\"Extract paper sections using pattern matching\"\"\"\n",
    "        sections = {}\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        section_starts = []\n",
    "        for match_id, start, end in matches:\n",
    "            if self.nlp.vocab.strings[match_id] == \"SECTION_HEADER\":\n",
    "                section_name = doc[start:end].text.lower()\n",
    "                section_starts.append((start, section_name))\n",
    "        \n",
    "        # Sort by position and extract content\n",
    "        section_starts.sort(key=lambda x: x[0])\n",
    "        \n",
    "        for i, (start, section_name) in enumerate(section_starts):\n",
    "            next_start = section_starts[i + 1][0] if i + 1 < len(section_starts) else len(doc)\n",
    "            section_content = doc[start:next_start].text\n",
    "            sections[section_name] = section_content\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _extract_citations(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"Extract in-text citations\"\"\"\n",
    "        citations = []\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            if self.nlp.vocab.strings[match_id] == \"CITATION\":\n",
    "                citations.append(doc[start:end].text)\n",
    "        \n",
    "        return list(set(citations))\n",
    "    \n",
    "    def _extract_references(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"Extract reference list (simplified)\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Look for references section\n",
    "        if \"references\" in self._extract_sections(doc):\n",
    "            ref_section = self._extract_sections(doc)[\"references\"]\n",
    "            ref_doc = self.nlp(ref_section)\n",
    "            \n",
    "            # Each sentence in references section is likely a reference\n",
    "            for sent in ref_doc.sents:\n",
    "                if len(sent.text.strip()) > 20:  # Filter out short lines\n",
    "                    references.append(sent.text.strip())\n",
    "        \n",
    "        return references\n",
    "    \n",
    "    def _extract_figures_tables(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"Extract figure and table references\"\"\"\n",
    "        fig_tables = []\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            if self.nlp.vocab.strings[match_id] == \"FIG_TABLE\":\n",
    "                fig_tables.append(doc[start:end].text)\n",
    "        \n",
    "        return list(set(fig_tables))\n",
    "    \n",
    "    def _extract_keywords(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"Extract keywords using noun phrases and NER\"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # Extract important noun phrases\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if len(chunk.text.split()) <= 3 and chunk.root.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                keywords.append(chunk.text.lower())\n",
    "        \n",
    "        # Extract technical terms (entities not recognized as standard types)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LANGUAGE\"]:\n",
    "                keywords.append(ent.text.lower())\n",
    "        \n",
    "        return list(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40b420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(document_type: str) -> DocumentParser:\n",
    "    \"\"\"Factory function to create appropriate parser based on document type\"\"\"\n",
    "    parsers = {\n",
    "        \"resume\": ResumeParser,\n",
    "        \"email\": EmailParser,\n",
    "        \"scientific\": ScientificPaperParser\n",
    "    }\n",
    "    \n",
    "    if document_type not in parsers:\n",
    "        raise ValueError(f\"Unknown document type: {document_type}\")\n",
    "    \n",
    "    return parsers[document_type]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cbf79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"../output-structured/email_1.md\"\n",
    "resume = \"../output-structured/resume_2.md\"\n",
    "scipub = \"../output-structured/scipub_1.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0ac1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(scipub, \"r\", encoding=\"utf-8\") as f:\n",
    "    file_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81c4a96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Post-radiolabelling for detecting DNA damage \\n\\n##### Mutagenesks vol.2 no.5 pp.319–331,1987\\n\\n## REVIEW \\n\\n### WilliamP.Watson \\n\\nShellRcitdRecr ME9 8AG.UK \\n\\n\\n\\n## I ntroduction \\n\\nThe biochemical and molecular basis of cancer continues to be an expanding area of research. Much of the driving force behind this important multi-disciplinary area is due to concerns about increasing human healthrisks caused by exposure to toxic chemicalsofbothindustrial andnaturalorigin.Insomeinsances the aetiological agents.for example vinyl chloride.as a cause of angiosarcomas in human liver,and tobaccosmoke asthemajor cause of lung cancer.have been identified byclinical and epidemiological studies involvinghigh-exposure poplations.There is.howeveran increasing nced for reliable prospective methods for predicting human cancer risks as a result ofchemical exposures.Thesomaticmutaionthoryofcancersuggests tat cancer is caused by genetic damage induced by chemicals or viruses. Such mutations resulting from covalent interactions of chemicalswith DNAmay be animportant initialstage inchemical carcinogenesis.There is now considerable evidence which.has establishedthatmancarcinogensortheirmetabolitescovalently bind to DNA in vivo with the formation of DNA adducs. The formationofDNAadductsbyindirectmechanismscanalsooccur with some carcinogens (Barrows and Shank,1981).Other alterations in DNAsuch asstrand breakage (Petzold and Swenberg.1978.genere-arrangement(Schimke.1982；Lavi1982)orene amplification(Caims,1981;Klein,1981;FeinbergandCofey 1982)may also be important stages in carcinogenesis.However.the covalent binding of chemicals to DNA appears to be the significantinitiatingeventwhichcanleadtosecondarygenomic alterationsunlessthecarcinogenadductisrepairedHencits widely accepted that anychemical which formscovalent bonds with DNA of somatic and reproductive cells in vivo should be viewedasa potential mutagen,carcinogen and teratogen.Studies on the binding of chemicals to DNA and the mechanisms of formation of DNA adducts are therefore important in the identification of potential carcinogens.in terms ofrisk assessment,amajor research objective is todetermine the qualitative and quantitative relationships:between the formation of DNA adducts and the resulting lesions in target tissues.\\n\\n\\n\\nMuch of our present knowledge on the interaction betwcen carcinogens and cellular macromolecules has been obtained by the use of isotopically labelled compounds.Thus,the detection and quantitation of the covalent binding of differentclasses of carcinogens to nucleic acids and proteins both in vitro and in vivo has been possibfe. From such studies Lutz (1979,1982)developed the concept of the covalent binding index which is defined as μmol. of chemical bound per mole of. DNA nucleotide/mmol ofchemical per kg body wt of animal.However.this concept does not take qualitative features into account and it is now generally accepted that covalent binding should be determined at the level of DNA adducts.Thus in most studies radiolabelledcarcinogenshavebeenadministeredtorodentsandthe presence of reaction products with DNA has been deermined afterisolationofDNAfromvarousorgaisuullyliver,follow edby hydrolysis of the DNA.The identity of DNA adducts in thehydrolysateisusuallydeteriedbycochromatgaphyof thehydrolysateonh.plc.withappropriatesynteticrerence compounds.InthiswayDNAadductsofvarietyofcarcingns including alkylatingagents,polycyclicaromatichydrocarbons.aromaticaminesandmycotoxinshave beenidentified.Modified DNAisusuallyenzymicallydigestedtoconstiuentdeoxrionucleosides which are then analysed for adducts.Some limitations can.arise due to incomplete digestion or instabilityof adducts.\\n\\n\\n\\nThe application of these techniques is limited to those compounds whicmaybesynthesizedreadilyinradiolabelledform andthespecificradioctivityacivabledetrminethlimtof detection of adducts in vivo.Studies in humans using these methods are not possible.As a result,alternative methods for studyinginerctinofcaciogesicDNAwhicdotely onradiolabelledcarcinogens have been developed.At the forefront of such methiods for detection of DNA adducts formed fromnon-radiolabelledcompoundsareimunochemicalass andpost-raiolabellinTincorporationofhig-spifica activityintheformofintoDNAadductshasthusfarfound widest application of the latter technique.\\n\\n\\n\\n## Post-labelling analysis of DNA \\n\\n### p.Post-labelling \\n\\nSeveral years ago,Randerath and his co-workers reported a new 3P-post-labellingmethodfortheanalysisofcarcinogen-DNA adducts(K.Randerathetal.1981,1984a,1985a;Guptat al..1982).TheprocedureisessentiallyadevelopmentofRandrahs methodology for base composition(K.Randerath et al,1980)and sequence analysis (K.Randerath etal.,1980;Maxam and Gilbent, 1981; Sanger.1981)of nucleic acids. The basic procedureinvolvetincorortionofintonn-radcive nucleicacidconstituentbyanenzyme-catalyedderivatization followedbychromatographic separationofradioactiveproducs.The most widelyusedchromatographic method has been multidimensionalthilayerchromatographytlc.)；thoughtheuse ofh.p.l.c.has also beenreported (Haseltine et al.,1983).Althoughseveralmodificationshavenowbeenincorporated intothe mthod,and these are describedbelow(andsummarized inguretesireisfollwAila fromtissuesofanimalsexposedtosuspectchemicalcarcinogens andthendigestedenzymicallytodeoxyriboucloide3monophosphates wichmicrococcal uclease andsplcenexonuclease.The deoxyrbonucleoside3-monophosphatesare thenconverted to thincorresonding5Plaelled35-bisphosphatebyen zymicderivatizationinvolvingP]phosphatetransferfrom [P]ATPusingT4polynuclotidekinase.T4polynucleotide kinase exhibitsabsolutespecificityforthetransferofP}phosphate fromATPto5-hydroxylgroupsofriboor deoxyribonucleoside3-monoposphatesandtirphosphodiester '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66679172",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_parser = create_parser(\"resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdab6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipub_parser = create_parser(\"scientific\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e7f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data = resume_parser.parse(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7beac9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = resume_parser.parse(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad1d2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipub_data = resume_parser.parse(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d24194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmailData(sender='', recipients=[], cc=[], bcc=[], subject='', date=None, body='John.Hoel@PMMC.comon 01 04 200109 41 27 PM To  Kim Tucker OAG@OAG cc Subject  RE  Today s Roll Call ImfreengsfinFloriontioructl Foolery.', attachments=[], sentiment='neutral', intent='informational', entities=[{'text': '01 04', 'label': 'DATE', 'start': 21, 'end': 26, 'confidence': ''}, {'text': '200109', 'label': 'DATE', 'start': 27, 'end': 33, 'confidence': ''}, {'text': '41 27 PM', 'label': 'TIME', 'start': 34, 'end': 42, 'confidence': ''}, {'text': 'Kim Tucker', 'label': 'PERSON', 'start': 47, 'end': 57, 'confidence': ''}])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8629f1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResumeData(personal_info=ContactInfo(emails=[], phones=['1966', '1971', '1971', '1974', '1967', '1970Reverend', '1970', '1973Fellow', '1989', '1992Academic', '1990', '1991', '1992Chairman', '1986', '1990', '1988', '1990Editor', '1983', '1991', '3715', '3719.Yen', '1991', '5077', '5081', '1991', '46264630'], addresses=[], websites=[], social_profiles={}), summary='', skills=[], experience=[], education=[], certifications=[], languages=[])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f1448f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResumeData(personal_info=ContactInfo(emails=[], phones=['1987', '1981', 'Swenberg.1978.genere', 'Schimke.1982', 'Lavi1982', '1981', '1981', '1982', '1979', '1982', 'K.Randerathetal.1981', '1984a', '1985a', '1982', '1980', '1980', '1981', 'Sanger.1981', '1983'], addresses=[], websites=['agents.for', 'chloride.as', 'cancer.have', 'is.howeveran', 'which.has', 'Swenberg.1978.genere', 'carcinogenesis.However.the', 'carcinogens.in', 'animal.However.this', 'thehydrolysateonh.plc.withappropriatesynteticrerence', 'polycyclicaromatichydrocarbons.aromaticaminesandmycotoxinshave', 'can.arise', 'ofh.p.l.c.has'], social_profiles={}), summary='', skills=[], experience=[], education=[], certifications=[], languages=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46e4a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ex = \"\"\"\n",
    "John Doe\n",
    "    Software Engineer\n",
    "    Email: john.doe@email.com\n",
    "    Phone: (555) 123-4567\n",
    "    \n",
    "    SUMMARY\n",
    "    Experienced software engineer with 5 years in web development.\n",
    "    \n",
    "    SKILLS\n",
    "    Python, JavaScript, React, Node.js, SQL, Git\n",
    "    \n",
    "    EXPERIENCE\n",
    "    Senior Developer at Tech Corp (2020-2023)\n",
    "    - Led development of web applications\n",
    "    - Managed team of 3 developers\n",
    "    \n",
    "    EDUCATION\n",
    "    BS Computer Science, State University (2018)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resume_parser = create_parser(\"resume\")\n",
    "resume_data = resume_parser.parse(text_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce0a455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResumeData(personal_info=ContactInfo(emails=['john.doe@email.com'], phones=['4567', '2020', '2023', '2018'], addresses=[], websites=['Node.js'], social_profiles={}), summary='', skills=['Python', 'Git'], experience=[{'company': 'Tech Corp  ', 'duration': ['2020-2023'], 'title': 'Developer', 'description': 'Senior Developer at Tech Corp  2020-2023  - Led development of web applications - Managed team of 3 developers'}], education=[{'institution': 'BS Computer Science  State University  ', 'graduation_date': '2018', 'description': 'BS Computer Science  State University  2018'}], certifications=[], languages=[])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "857f0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEnhancer:\n",
    "    \"\"\"LLM integration for semantic understanding and advanced extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: LLMProvider = LLMProvider.LOCAL, model_name: str = \"llama3\"):\n",
    "        self.provider = provider\n",
    "        self.model_name = model_name\n",
    "        self._setup_client()\n",
    "    \n",
    "    def _setup_client(self):\n",
    "        \"\"\"Setup LLM client based on provider\"\"\"\n",
    "        if self.provider == LLMProvider.LOCAL:\n",
    "            # For local models like Ollama\n",
    "            self.client = None  # Initialize your local LLM client\n",
    "        elif self.provider == LLMProvider.OPENAI:\n",
    "            # import openai\n",
    "            # self.client = openai.OpenAI(api_key=\"your-key\")\n",
    "            pass\n",
    "        # Add other providers as needed\n",
    "    \n",
    "    async def analyze_with_llm(self, text: str, prompt_template: str, schema: dict = None) -> dict[str, any]:\n",
    "        \"\"\"Generic LLM analysis method\"\"\"\n",
    "        # This is a placeholder - implement based on your LLM provider\n",
    "        # For demonstration purposes\n",
    "        \n",
    "        prompt = prompt_template.format(text=text)\n",
    "        \n",
    "        # Mock response structure - replace with actual LLM call\n",
    "        if \"resume\" in prompt_template.lower():\n",
    "            return self._mock_resume_analysis()\n",
    "        elif \"email\" in prompt_template.lower():\n",
    "            return self._mock_email_analysis()\n",
    "        elif \"scientific\" in prompt_template.lower():\n",
    "            return self._mock_scientific_analysis()\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _mock_resume_analysis(self) -> dict[str, any]:\n",
    "        \"\"\"Mock LLM resume analysis - replace with actual implementation\"\"\"\n",
    "        return {\n",
    "            \"skills_categorized\": {\n",
    "                \"technical\": [\"Python\", \"Machine Learning\", \"SQL\"],\n",
    "                \"soft\": [\"Leadership\", \"Communication\", \"Problem Solving\"],\n",
    "                \"domain\": [\"Healthcare\", \"Fintech\"]\n",
    "            },\n",
    "            \"achievement_metrics\": [\n",
    "                {\"achievement\": \"Increased system performance\", \"metric\": \"40%\", \"impact\": \"high\"}\n",
    "            ],\n",
    "            \"career_progression\": {\n",
    "                \"trend\": \"upward\",\n",
    "                \"leadership_growth\": True,\n",
    "                \"technical_depth\": \"increasing\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _mock_email_analysis(self) -> dict[str, any]:\n",
    "        \"\"\"Mock LLM email analysis\"\"\"\n",
    "        return {\n",
    "            \"emotional_tone\": {\"professional\": 0.8, \"urgent\": 0.3, \"friendly\": 0.6},\n",
    "            \"intent_hierarchy\": [\n",
    "                {\"intent\": \"request_information\", \"confidence\": 0.9},\n",
    "                {\"intent\": \"schedule_meeting\", \"confidence\": 0.7}\n",
    "            ],\n",
    "            \"action_items\": [\"Provide quarterly report\", \"Schedule follow-up meeting\"]\n",
    "        }\n",
    "    \n",
    "    def _mock_scientific_analysis(self) -> dict[str, any]:\n",
    "        \"\"\"Mock LLM scientific paper analysis\"\"\"\n",
    "        return {\n",
    "            \"research_contribution\": {\n",
    "                \"type\": \"methodological\",\n",
    "                \"novelty\": \"moderate\",\n",
    "                \"significance\": \"high\"\n",
    "            },\n",
    "            \"methodology_type\": \"experimental\",\n",
    "            \"research_gaps_identified\": [\"Limited sample size\", \"Geographic bias\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "148012da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDocumentParser:\n",
    "    \"\"\"Hybrid parser combining SpaCy efficiency with LLM intelligence\"\"\"\n",
    "    \n",
    "    def __init__(self, use_llm: bool = True, llm_provider: LLMProvider = LLMProvider.LOCAL):\n",
    "        # Initialize SpaCy components\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.phrase_matcher = PhraseMatcher(self.nlp.vocab)\n",
    "        \n",
    "        # Initialize LLM enhancer\n",
    "        self.use_llm = use_llm\n",
    "        if use_llm:\n",
    "            self.llm_enhancer = LLMEnhancer(provider=llm_provider)\n",
    "        \n",
    "        self._setup_patterns()\n",
    "    \n",
    "    def _setup_patterns(self):\n",
    "        \"\"\"Setup SpaCy patterns for fast extraction\"\"\"\n",
    "        # Email patterns\n",
    "        email_pattern = [{\"LIKE_EMAIL\": True}]\n",
    "        self.matcher.add(\"EMAIL\", [email_pattern])\n",
    "        \n",
    "        # Phone patterns\n",
    "        phone_patterns = [\n",
    "            [{\"SHAPE\": \"ddd-ddd-dddd\"}],\n",
    "            [{\"SHAPE\": \"(ddd) ddd-dddd\"}]\n",
    "        ]\n",
    "        self.matcher.add(\"PHONE\", phone_patterns)\n",
    "        \n",
    "        # Skills patterns (common technical terms)\n",
    "        tech_skills = [\"python\", \"javascript\", \"sql\", \"react\", \"kubernetes\", \"aws\"]\n",
    "        skill_patterns = [self.nlp(skill) for skill in tech_skills]\n",
    "        self.phrase_matcher.add(\"TECH_SKILLS\", skill_patterns)\n",
    "    \n",
    "    async def parse_resume(self, text: str, enhanced: bool = True) -> dict | EnhancedResumeData:\n",
    "        \"\"\"Parse resume with optional LLM enhancement\"\"\"\n",
    "        # Phase 1: Fast SpaCy extraction\n",
    "        doc = self.nlp(text)\n",
    "        basic_data = self._extract_basic_resume_data(doc)\n",
    "        \n",
    "        if not enhanced or not self.use_llm:\n",
    "            return basic_data\n",
    "        \n",
    "        # Phase 2: LLM enhancement for complex understanding\n",
    "        llm_insights = await self._enhance_resume_with_llm(text, basic_data)\n",
    "        \n",
    "        return EnhancedResumeData(\n",
    "            basic_info=basic_data,\n",
    "            **llm_insights\n",
    "        )\n",
    "    \n",
    "    async def parse_email(self, text: str, enhanced: bool = True) -> dict | EnhancedEmailData:\n",
    "        \"\"\"Parse email with optional LLM enhancement\"\"\"\n",
    "        # Phase 1: SpaCy structure extraction\n",
    "        doc = self.nlp(text)\n",
    "        basic_data = self._extract_basic_email_data(doc)\n",
    "        \n",
    "        if not enhanced or not self.use_llm:\n",
    "            return basic_data\n",
    "        \n",
    "        # Phase 2: LLM semantic analysis\n",
    "        llm_insights = await self._enhance_email_with_llm(text, basic_data)\n",
    "        \n",
    "        return EnhancedEmailData(\n",
    "            basic_structure=basic_data,\n",
    "            **llm_insights\n",
    "        )\n",
    "    \n",
    "    async def parse_scientific_paper(self, text: str, enhanced: bool = True) -> dict | EnhancedScientificData:\n",
    "        \"\"\"Parse scientific paper with optional LLM enhancement\"\"\"\n",
    "        # Phase 1: SpaCy structural extraction\n",
    "        doc = self.nlp(text)\n",
    "        basic_data = self._extract_basic_scientific_data(doc)\n",
    "        \n",
    "        if not enhanced or not self.use_llm:\n",
    "            return basic_data\n",
    "        \n",
    "        # Phase 2: LLM research intelligence\n",
    "        llm_insights = await self._enhance_scientific_with_llm(text, basic_data)\n",
    "        \n",
    "        return EnhancedScientificData(\n",
    "            basic_structure=basic_data,\n",
    "            **llm_insights\n",
    "        )\n",
    "    \n",
    "    def _extract_basic_resume_data(self, doc) -> dict[str, any]:\n",
    "        \"\"\"Fast SpaCy-based resume extraction\"\"\"\n",
    "        data = {\n",
    "            \"emails\": [],\n",
    "            \"phones\": [],\n",
    "            \"skills\": [],\n",
    "            \"organizations\": [],\n",
    "            \"dates\": []\n",
    "        }\n",
    "        \n",
    "        # Pattern matching for structured data\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            label = self.nlp.vocab.strings[match_id]\n",
    "            span_text = doc[start:end].text\n",
    "            \n",
    "            if label == \"EMAIL\":\n",
    "                data[\"emails\"].append(span_text)\n",
    "            elif label == \"PHONE\":\n",
    "                data[\"phones\"].append(span_text)\n",
    "        \n",
    "        # Entity extraction\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                data[\"organizations\"].append(ent.text)\n",
    "            elif ent.label_ == \"DATE\":\n",
    "                data[\"dates\"].append(ent.text)\n",
    "        \n",
    "        # Phrase matching for skills\n",
    "        phrase_matches = self.phrase_matcher(doc)\n",
    "        for match_id, start, end in phrase_matches:\n",
    "            if self.nlp.vocab.strings[match_id] == \"TECH_SKILLS\":\n",
    "                data[\"skills\"].append(doc[start:end].text)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _extract_basic_email_data(self, doc) -> dict[str, any]:\n",
    "        \"\"\"Fast SpaCy-based email extraction\"\"\"\n",
    "        return {\n",
    "            \"entities\": [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents],\n",
    "            \"sentences\": [sent.text for sent in doc.sents],\n",
    "            \"emails\": [token.text for token in doc if token.like_email],\n",
    "            \"word_count\": len([token for token in doc if not token.is_space])\n",
    "        }\n",
    "    \n",
    "    def _extract_basic_scientific_data(self, doc) -> dict[str, any]:\n",
    "        \"\"\"Fast SpaCy-based scientific paper extraction\"\"\"\n",
    "        return {\n",
    "            \"entities\": [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents],\n",
    "            \"noun_phrases\": [chunk.text for chunk in doc.noun_chunks],\n",
    "            \"sentences\": [sent.text for sent in doc.sents[:10]],  # First 10 sentences\n",
    "            \"organizations\": [ent.text for ent in doc.ents if ent.label_ == \"ORG\"],\n",
    "            \"people\": [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "        }\n",
    "    \n",
    "    async def _enhance_resume_with_llm(self, text: str, basic_data: dict) -> dict[str, any]:\n",
    "        \"\"\"Use LLM for advanced resume analysis\"\"\"\n",
    "        \n",
    "        resume_prompt = \"\"\"\n",
    "        Analyze this resume text and provide insights in JSON format:\n",
    "        \n",
    "        Resume Text: {text}\n",
    "        \n",
    "        Please categorize skills, extract achievement metrics, assess career progression,\n",
    "        and identify personality traits. Focus on semantic understanding and context.\n",
    "        \n",
    "        Return structured JSON with skills_categorized, achievement_metrics, career_progression fields.\n",
    "        \"\"\"\n",
    "        \n",
    "        return await self.llm_enhancer.analyze_with_llm(text, resume_prompt)\n",
    "    \n",
    "    async def _enhance_email_with_llm(self, text: str, basic_data: dict) -> dict[str, any]:\n",
    "        \"\"\"Use LLM for advanced email analysis\"\"\"\n",
    "        \n",
    "        email_prompt = \"\"\"\n",
    "        Analyze this email for emotional tone, intent hierarchy, and action items:\n",
    "        \n",
    "        Email Text: {text}\n",
    "        \n",
    "        Provide emotional tone scores, ranked intents, extracted action items,\n",
    "        and assess urgency and relationship context.\n",
    "        \n",
    "        Return JSON with emotional_tone, intent_hierarchy, action_items fields.\n",
    "        \"\"\"\n",
    "        \n",
    "        return await self.llm_enhancer.analyze_with_llm(text, email_prompt)\n",
    "    \n",
    "    async def _enhance_scientific_with_llm(self, text: str, basic_data: dict) -> dict[str, any]:\n",
    "        \"\"\"Use LLM for advanced scientific paper analysis\"\"\"\n",
    "        \n",
    "        scientific_prompt = \"\"\"\n",
    "        Analyze this scientific paper excerpt for research contribution and methodology:\n",
    "        \n",
    "        Paper Text: {text}\n",
    "        \n",
    "        Assess the research contribution type, methodology, novelty, and identify\n",
    "        research gaps and future work suggestions.\n",
    "        \n",
    "        Return JSON with research_contribution, methodology_type, research_gaps_identified fields.\n",
    "        \"\"\"\n",
    "        \n",
    "        return await self.llm_enhancer.analyze_with_llm(text, scientific_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ddb9c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def choose_strategy(mode: str, file_text: str, parser: HybridDocumentParser) -> str:\n",
    "    \"\"\"\n",
    "    Choose parsing strategy based on requirements:\n",
    "    \"\"\"\n",
    "        \n",
    "    match mode:\n",
    "        case \"quick\":\n",
    "            parsed_data = await parser.parse_resume(file_text, enhanced=False)\n",
    "            return parsed_data\n",
    "            \n",
    "        case \"enhanced\":\n",
    "            try:\n",
    "                parsed_data = await parser.parse_resume(file_text, enhanced=True)\n",
    "                return parsed_data\n",
    "            except Exception as e:\n",
    "                print(f\"exception occured {e}\")\n",
    "                parsed_data = await parser.parse_resume(file_text, enhanced=False)\n",
    "                return parsed_data\n",
    "            \n",
    "        case _:\n",
    "            raise Exception(\"UnSupported mode used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef14b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hybrid parser\n",
    "parser = HybridDocumentParser(use_llm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e18868a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic extraction: {'emails': [], 'phones': [], 'skills': [], 'organizations': ['formationofDNAadductsbyindirectmechanismscanalsooccur', 'DNAsuch', 'mutagen', 'carcinogen', 'deoxyribonucleoside3'], 'dates': ['Several years ago', '1981']}\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: SpaCy only (fast, basic)\n",
    "basic_result = await parser.parse_resume(file_text, enhanced=False)\n",
    "print(\"Basic extraction:\", basic_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddae30f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced extraction: {'technical': ['Python', 'Machine Learning', 'SQL'], 'soft': ['Leadership', 'Communication', 'Problem Solving'], 'domain': ['Healthcare', 'Fintech']}\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Full LLM enhancement (slow, comprehensive)\n",
    "enhanced_result = await parser.parse_resume(file_text, enhanced=True)\n",
    "print(\"Enhanced extraction:\", enhanced_result.skills_categorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ea16ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended strategy: EnhancedResumeData(basic_info={'emails': [], 'phones': [], 'skills': [], 'organizations': ['formationofDNAadductsbyindirectmechanismscanalsooccur', 'DNAsuch', 'mutagen', 'carcinogen', 'deoxyribonucleoside3'], 'dates': ['Several years ago', '1981']}, skills_categorized={'technical': ['Python', 'Machine Learning', 'SQL'], 'soft': ['Leadership', 'Communication', 'Problem Solving'], 'domain': ['Healthcare', 'Fintech']}, experience_insights=[], achievement_metrics=[{'achievement': 'Increased system performance', 'metric': '40%', 'impact': 'high'}], career_progression={'trend': 'upward', 'leadership_growth': True, 'technical_depth': 'increasing'}, personality_traits=[], industry_fit={})\n"
     ]
    }
   ],
   "source": [
    "strategy = await choose_strategy(\n",
    "    mode=\"enhanced\",\n",
    "    file_text=file_text,\n",
    "    parser=parser\n",
    ")\n",
    "print(f\"Recommended strategy: {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e7bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
